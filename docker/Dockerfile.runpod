# RunPod Optimized Dockerfile  
# Pre-built with models, ready for immediate use, optimized for GPU

FROM nvidia/cuda:11.8-runtime-ubuntu22.04

# Install Python 3.10
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3.10-venv \
    python3.10-dev \
    python3-pip \
    ffmpeg \
    git \
    wget \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create symlinks for python
RUN ln -sf /usr/bin/python3.10 /usr/bin/python3
RUN ln -sf /usr/bin/python3.10 /usr/bin/python

# Set environment variables for RunPod
ENV PYTHONPATH="/app"
ENV CUDA_VISIBLE_DEVICES="0"
ENV TORCH_CUDA_ARCH_LIST="6.0;6.1;7.0;7.5;8.0;8.6+PTX"
ENV GRADIO_SERVER_NAME="0.0.0.0"
ENV GRADIO_SERVER_PORT=7860

# Set working directory
WORKDIR /app

# Copy requirements and install dependencies with CUDA support
COPY requirements.txt .
RUN pip install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
RUN pip install --no-cache-dir -r requirements.txt

# Copy the complete application
COPY . .

# Pre-download models (this makes the container larger but faster to start)
RUN python3 -c "
import torch
from transformers import AutoTokenizer
# Pre-cache common models
try:
    print('Pre-downloading models...')
    AutoTokenizer.from_pretrained('microsoft/DialoGPT-medium')
    print('Models cached successfully')
except Exception as e:
    print(f'Model pre-caching failed: {e}')
"

# Create necessary directories
RUN mkdir -p Text_Input Voice_Samples Output Audiobook logs

# Set permissions
RUN chmod +x launch*.sh

# Expose port
EXPOSE 7860

# Default command - can be overridden
CMD ["python3", "gradio_main_interface.py"]