# ChatterboxTTS Enhancement Guide - Detailed Implementation

## **1. Voice Sample Quality Enhancement**

### **Multiple voice samples for better conditioning**
**What**: Instead of one ref.wav, use 3-5 different samples of the same voice
**Benefit**: Better voice consistency, captures more voice characteristics
**Implementation**: Check if `model.prepare_conditionals()` accepts list of files instead of single file

### **Longer samples (30+ seconds)**
**What**: Use longer reference audio clips
**Benefit**: More voice data = better cloning quality, captures speech patterns
**Implementation**: Edit your current ref.wav files to be longer, test quality difference

### **Clean audio preprocessing**
**What**: Clean reference audio before using it
**Benefit**: Removes artifacts that contaminate voice cloning
**Implementation**: Use FFmpeg/Audacity to denoise, normalize volume, remove silence gaps

### **Emotional variety in reference samples**
**What**: Create happy/sad/neutral versions of same voice
**Benefit**: Better emotional range matching your VADER analysis
**Implementation**: Pitch shift (+/-10%), speed adjust (+/-5%) existing ref.wav

## **2. VADER Parameter Tuning**

### **How to Fine-tune VADER Sensitivity**
**Current location**: `config/config.py` - look for variables like:
- `VADER_EXAGGERATION_SENSITIVITY = 0.3`  
- `VADER_CFG_WEIGHT_SENSITIVITY = 0.2`
- `VADER_TEMPERATURE_SENSITIVITY = 0.1`

**Testing approach**:
1. Process same book with different sensitivity values
2. Listen to emotional transitions between chunks
3. Find sweet spot where emotions change naturally but not jarringly

**Benefit**: More natural emotional flow, less robotic transitions

## **3. Multiple Reference Samples Support**

### **How to determine if ChatterboxTTS supports it**
**Check**: Look at `src/chatterbox/tts.py:191` in `prepare_conditionals()` method
- Does `wav_fpath` accept a list?
- Can you call it multiple times to build better conditioning?

**Test**: Try passing array of files instead of single file, see if it errors

**Benefit**: Richer voice representation, better quality output

## **4. Model Performance Optimization**

### **Pre-warm models with voice samples**
**What**: Load and process voice sample once before starting book processing
**Benefit**: Eliminates "cold start" quality issues, first chunk sounds as good as later ones
**Implementation**: Call `model.prepare_conditionals()` with dummy text before real processing

### **Cache voice embeddings between chunks**
**What**: Save computed voice features, reuse instead of recalculating
**Benefit**: Faster processing, consistent voice characteristics across chunks
**Implementation**: Store `self.conds` object, reuse for similar emotional chunks

### **Batch similar emotional chunks together**
**What**: Process all "happy" chunks together, then all "sad" chunks
**Benefit**: Model stays in emotional "mode", better consistency within emotions
**Implementation**: Sort chunks by VADER score before processing, group by emotion ranges

## **5. VADER Smoothing Enhancement**

### **Adaptive window sizes based on content type**
**What**: Use different smoothing for dialogue vs narration vs action scenes
**Benefit**: Dialogue changes emotion quickly, narration changes slowly
**Implementation**: Detect content type (quotes = dialogue), adjust smoothing window

### **Chapter-aware smoothing**
**What**: Don't smooth VADER scores across chapter boundaries
**Benefit**: Each chapter can have different emotional baseline
**Implementation**: Reset smoothing when `boundary_type = "chapter_start"`

### **Character dialogue detection**
**What**: Different emotional processing for different characters
**Benefit**: Each character has consistent voice personality
**Implementation**: Parse quoted text, assign character IDs, separate VADER processing

## **6. Advanced Conditioning**

### **Emotional conditioning with different ref samples**
**What**: Use happy-ref.wav for positive VADER scores, sad-ref.wav for negative
**Benefit**: Voice actually sounds different emotionally, not just parameter changes
**Implementation**: 
```python
if vader_score > 0.3:
    ref_file = "happy_ref.wav"
elif vader_score < -0.3:
    ref_file = "sad_ref.wav"
else:
    ref_file = "neutral_ref.wav"
```

### **Context-aware voice selection**
**What**: Different voice characteristics for different book sections
**Benefit**: Narrator voice vs character voices, chapter-specific tones
**Implementation**: Detect chapter titles, character names, switch reference accordingly

## **7. Quality Improvements**

### **Better hum detection algorithms**
**What**: Improve detection of TTS artifacts (humming, buzzing)
**Benefit**: Catch more quality issues, fewer bad chunks
**Implementation**: Analyze more frequency ranges, use ML-based detection

### **Spectral cleaning of generated audio**
**What**: Remove frequency artifacts after TTS generation
**Benefit**: Cleaner, more professional sounding audio
**Implementation**: Apply FFmpeg filters or librosa spectral subtraction

### **Consistency matching between chunks**
**What**: Ensure volume, tone, pace consistent across chunks
**Benefit**: Seamless audiobook experience, no jarring transitions
**Implementation**: Analyze first/last 0.5 seconds of each chunk, match characteristics

## **Priority Order for Implementation**

### **Start with** (easiest, biggest impact):
1. **VADER parameter tuning** - just change config values, test
2. **Longer reference samples** - edit existing files, test quality
3. **Pre-warm models** - add one function call, immediate consistency improvement

### **Then try** (moderate effort):
4. **Emotional reference samples** - create happy/sad versions, map to VADER
5. **Chapter-aware smoothing** - modify existing smoothing code

### **Advanced** (more complex):
6. **Multiple reference support** - requires understanding ChatterboxTTS internals
7. **Advanced conditioning** - needs content analysis and dynamic voice switching

## **Implementation Notes**

### **Current GUI Limitations**
- Voice sample analysis in GUI is rudimentary
- Enhancement options not sophisticated enough
- Autofix with noise reduction causes clicking noises in output
- Need more advanced audio preprocessing pipeline

### **Key File Locations**
- VADER sensitivity parameters: `config/config.py`
- TTS conditioning: `src/chatterbox/tts.py:191` (`prepare_conditionals()`)
- Chunk processing: `modules/tts_engine.py`
- Voice embedding caching: `self.conds` object in ChatterboxTTS class

### **Testing Strategy**
- Use same book/voice for A/B testing different parameters
- Focus on emotional transition quality between chunks
- Monitor for consistency across long audiobooks
- Document optimal parameter combinations for different voice types

# Advanced Voice Sample Preparation System

## Automated Emotional Voice Sample Extraction

### **Problem Statement**
Current ChatterboxTTS uses single static voice samples for all emotional content. Manual curation of emotional samples from narrators is impractical. Need automated system to analyze 30-120 minute audiobooks and extract optimal emotional voice samples for different states.

### **System Architecture**

#### **Input Processing Pipeline**
1. **Audio Quality Analysis**
   - Sample rate detection (22kHz, 44.1kHz, 48kHz typical)
   - Bitrate assessment (64kbps poor, 96kbps acceptable, 128kbps+ good)
   - Quality metrics: SNR, dynamic range, clipping detection, noise floor
   - Conservative cleanup: DC removal, gentle normalization, high-pass filtering

2. **Text Extraction Strategy**
   - **Primary**: Audiobook + Ebook forced alignment (preferred for quality)
   - **Fallback**: Whisper large-v3 ASR with chunked processing for 8GB VRAM
   - **Manual assistance**: Formatting tools for ASR output

3. **Advanced Emotion Analysis** 
   - Replace VADER (1 dimension) with GoEmotions (27 categories)
   - Multi-dimensional models: Plutchik's Wheel, Russell's Circumplex
   - Content type detection: dialogue vs narration vs description
   - Narrative context analysis: character emotions vs narrator tone

#### **8GB VRAM Optimization Strategy**
Based on successful XTTS fine-tuning approach with smaller batches:

```python
def transcribe_large_audio_chunked(audio_path, chunk_minutes=10):
    """
    Process 2-hour audiobook in 10-minute chunks for 8GB VRAM constraint
    Similar to XTTS fine-tuning batch optimization approach
    """
    # Chunked processing: 10-15 minute segments
    # Progressive fallback: 15min → 10min → 5min if OOM
    # Memory management: torch.cuda.empty_cache() between chunks
    # Expected performance: 0.05-0.1x realtime (2hr book = 20-40 minutes)
```

#### **Emotional Sample Library Structure**
```
Voice_Samples/
├── narrator_name/
│   ├── neutral.wav (10 sec)
│   ├── happy.wav (10 sec)
│   ├── sad.wav (10 sec)
│   ├── angry.wav (10 sec)
│   ├── thoughtful.wav (10 sec)
│   ├── fearful.wav (10 sec)
│   └── voice_profile.json
```

#### **Voice Profile Optimization**
```json
{
  "voice_name": "narrator_01",
  "extraction_confidence": 0.89,
  "samples": {
    "neutral": {
      "file": "neutral.wav",
      "vader_range": [-0.2, 0.2],
      "optimal_params": {
        "exaggeration": 0.4,
        "temperature": 0.7,
        "cfg_weight": 0.5
      }
    }
  }
}
```

### **Dynamic Voice Sample Switching**

#### **Implementation Approaches**
1. **Chunk-level switching**: Perfect emotional matching, potential inconsistency
2. **Grouped processing**: Better consistency, more efficient (recommended)
3. **Hybrid JSON files**: Split by emotional category, process separately

#### **Integration with Existing Pipeline**
- **JSON generation**: Enhanced emotion analysis, same structure
- **Chunk processing**: Same `prepare_conditionals()` calls, different wav files
- **All interfaces preserved**: Gradio, CLI, GUI work unchanged
- **One-time setup**: 30 minutes per voice, ongoing benefits

### **Conservative Audio Cleanup Pipeline**

#### **Safe Enhancement Methods**
```python
def conservative_audio_cleanup(audio_path):
    """
    Voice-preserving audio improvements
    - DC offset removal (always safe)
    - Gentle normalization (preserve dynamics) 
    - High-pass filter (remove rumble <80Hz)
    - Mild noise reduction (only if SNR <10)
    - Soft limiting (prevent clipping)
    """
```

#### **Quality Decision Tree**
- Quality score >0.8: Minimal processing only
- SNR <15: Add spectral subtraction
- Clipping detected: Attempt declipping
- Low dynamic range: Gentle expansion

### **Advanced Emotion Classification**

#### **Beyond VADER Limitations**
- **Current**: Single sentiment dimension (-1 to +1)
- **Proposed**: 27 emotion categories (GoEmotions)
- **Additional**: Arousal, dominance, intensity levels
- **Context-aware**: Character dialogue vs narrator voice

#### **Literary Analysis Integration**
- Content type detection: dialogue, narration, action, introspection
- Character emotion vs narrator tone separation
- Scene context: tension level, pacing, point of view
- Sentence complexity and formality analysis

### **Hardware-Optimized Processing**

#### **8GB VRAM Constraints Applied**
Lessons from XTTS fine-tuning success:
- **Chunked processing**: 10-15 minute audio segments
- **Batch optimization**: Multiple epochs → multiple chunks approach
- **Memory management**: Aggressive cleanup between chunks
- **Progressive fallback**: Reduce chunk size if OOM occurs
- **Model reuse**: Load once, process multiple chunks

#### **Expected Performance**
- **Setup time**: 20-40 minutes per 2-hour audiobook
- **VRAM usage**: 6-7GB peak per chunk
- **Quality**: Same as full large-v3 model
- **Output**: 6-8 emotional voice samples + optimized parameters

### **User Requirements Integration**

#### **Audio Quality Considerations**
- Input formats: 64-192kbps, various sample rates
- Quality assessment before processing
- Conservative cleanup to preserve voice characteristics
- Fallback to existing audio if cleanup fails

#### **Text Source Flexibility**
- **Preferred**: Audiobook + matching ebook (forced alignment)
- **Alternative**: ASR-only with Whisper large-v3
- **Manual tools**: Dialogue formatting assistance for ASR output
- **Quality validation**: Compare ASR with ebook when available

#### **One-Time Processing Philosophy**
- Time investment acceptable for quality improvement
- Reusable voice libraries across multiple audiobooks
- Investment in infrastructure vs per-book processing
- Quality over speed for voice preparation phase

# Multi-Tier Voice Sample System Architecture

## Comprehensive 4-Tier Approach

### **Tier 1: Automated Extraction (Best Quality)**
**Standalone program**: `chatterbox-voice-extractor`
- Input: 30-120 min audiobook + optional ebook
- Output: 6-8 emotional voice samples (10 sec each)
- Shareable via separate git repo
- Invokable from main program for integration

### **Tier 2: Existing Clip Analysis (Fallback)**
**For static samples only**:
- Analyze existing 5-10 minute samples for emotional segments
- Extract best 10-second clips for each sentiment
- Quality scoring and confidence metrics
- Semi-automated with manual review

### **Tier 3: Sample Alteration (Enhancement)**
**Transform existing clips for missing emotions**:

#### **Whole-clip alterations**:
- **Pitch**: +10-15% for happy/excited, -10-15% for sad/thoughtful
- **Tempo**: +15-20% for energetic, -15-20% for contemplative
- **Formant shifting**: Subtle voice character changes
- **Dynamic range**: Compress for intimacy, expand for drama

#### **Partial-sentence alterations**:
Advanced prosodic manipulation targeting specific sentence segments:
- **Beginning emphasis**: Attack characteristics for sentence starts
- **Middle modulation**: Sustained emotional characteristics  
- **End shaping**: Rising/falling contours for questions/statements
- **Contextual processing**: Question rising, statement falling, excitement sustained

### **Tier 4: Synthetic Generation (Direct TTS Control)**

#### **Core Philosophy**
Skip complex sentiment analysis pipeline entirely:
- ❌ No VADER analysis required
- ❌ No sentiment smoothing needed  
- ❌ No complex emotion classification
- ✅ Direct TTS parameter control
- ✅ ChatterboxTTS built-in text parsing
- ✅ Real-time parameter adjustment

#### **Preset Parameter Library**
```python
EMOTIONAL_TTS_PRESETS = {
    "neutral": {
        "exaggeration": 0.4, "cfg_weight": 0.5, "temperature": 0.7,
        "min_p": 0.05, "top_p": 0.8, "repetition_penalty": 2.0
    },
    "happy": {
        "exaggeration": 0.8, "cfg_weight": 0.6, "temperature": 0.9,
        "min_p": 0.03, "top_p": 0.85, "repetition_penalty": 1.8
    },
    "sad": {
        "exaggeration": 0.6, "cfg_weight": 0.4, "temperature": 0.6,
        "min_p": 0.08, "top_p": 0.75, "repetition_penalty": 2.2
    },
    "angry": {
        "exaggeration": 0.9, "cfg_weight": 0.7, "temperature": 0.8,
        "min_p": 0.02, "top_p": 0.9, "repetition_penalty": 1.5
    }
}
```

#### **Optimized Emotional Text Prompts**
```python
OPTIMIZED_EMOTIONAL_PROMPTS = {
    "happy": "What a delightful surprise this turned out to be! I'm absolutely thrilled!",
    "sad": "Her heart grew heavy with sorrow, and quiet tears began to fall.",
    "angry": "This is completely unacceptable! I refuse to tolerate such behavior!",
    "fearful": "Something lurked in the darkness ahead... what if they found us?",
    "thoughtful": "Perhaps we should carefully consider all possible alternatives here.",
    "excited": "This is incredible! The moment we've been waiting for is finally here!"
}
```

#### **Synthetic Generation UI Design**
Interactive parameter tuning interface:
- **Emotion preset selector**: Loads base parameters for selected emotion
- **Parameter spinners**: Real-time adjustment of exaggeration, cfg_weight, temperature, etc.
- **Generate & Play**: Instant audio generation and playback
- **A/B Testing**: Compare multiple parameter combinations
- **Save to Library**: Store best results with optimal parameters

#### **Generation Workflow**
```
1. Select Emotion Preset → Load base TTS parameters
2. Fine-tune Parameters → Real-time spinner adjustments  
3. Generate Sample → Using dnxs-spokenword + emotional prompt
4. A/B Test → Compare against previous versions
5. Save Best → Store to voice library with parameters
```

#### **Parameter Discovery Process**
Systematic exploration around base parameter values:
- Test parameter ranges: ±0.1, ±0.2 from preset values
- Generate samples with different combinations
- Rate emotional authenticity (user or automated)
- Save optimal parameter combinations per emotion

#### **Integration with dnxs-spokenword**
Leverage existing TTS system for consistency:
- Same voice characteristics as main pipeline
- Controlled emotional text content
- Predictable generation quality
- Known ChatterboxTTS capabilities

## Standalone Program Architecture

### **`chatterbox-voice-extractor/` Structure**
```
├── src/
│   ├── audio_analysis.py       # Quality assessment, cleanup
│   ├── emotion_extraction.py   # Advanced emotion detection  
│   ├── whisper_chunked.py      # 8GB VRAM optimized transcription
│   ├── sample_alteration.py    # Pitch/tempo/formant manipulation
│   └── synthetic_generation.py # Direct TTS parameter control
├── models/                     # Emotion classification models
├── config/
│   ├── emotion_prompts.json    # Text prompts for synthetic generation
│   ├── tts_presets.json       # Base parameters for each emotion
│   └── extraction_params.json # Default processing parameters
├── ui/
│   ├── synthetic_generator.py  # GUI for parameter tuning
│   └── sample_manager.py      # Voice library management
├── cli.py                      # Command-line interface
└── integration_api.py          # API for main program integration
```

### **Quality Hierarchy Implementation**
**Best → Good → Acceptable → Fallback**:
1. **Automated extraction** from professional audiobooks (Tier 1)
2. **Manual selection** from existing long samples (Tier 2)  
3. **Altered samples** with prosodic modifications (Tier 3)
4. **Synthetic generation** with optimized TTS parameters (Tier 4)

### **Voice Library Output Structure**
```
Voice_Samples/narrator_01/
├── happy.wav (10 sec, optimal emotional expression)
├── sad.wav
├── angry.wav
├── voice_profile.json (includes generation method & parameters)
└── generation_log.json (parameter discovery history)
```

### **Advanced Parameter Optimization**
- **Real-time feedback**: Immediate audio generation and playback
- **Comparative testing**: A/B comparison between parameter sets  
- **Progressive refinement**: Iterative improvement of emotional expression
- **Reproducible results**: Exact parameters saved for each emotion
- **Consistent voice**: Same TTS system maintains voice characteristics

## System Integration Benefits

### **Standalone Architecture Advantages**
- **Separate git repository**: Shareable voice extraction system
- **Modular design**: Can be used independently or integrated
- **Focused functionality**: Specialized for voice sample preparation
- **Reusable components**: Voice libraries work across projects

### **Main Program Integration**
- **Seamless workflow**: Same `prepare_conditionals()` interface
- **Enhanced emotional range**: Multiple voice samples per speaker
- **Preserved interfaces**: Gradio, CLI, GUI work unchanged
- **Quality hierarchy**: Automatic fallback through tiers

### **Processing Efficiency**
- **One-time investment**: 30-60 minutes setup per voice
- **Ongoing benefits**: Reusable across unlimited audiobooks  
- **Direct control**: No black-box sentiment analysis overhead
- **Consistent results**: Reproducible parameter combinations